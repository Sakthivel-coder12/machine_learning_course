{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d474b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Functions and Derivatives\n",
    "def relu(z): return np.maximum(0, z)\n",
    "def relu_derivative(z): return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_derivative(z): s = sigmoid(z); return s * (1 - s)\n",
    "\n",
    "def tanh(z): return np.tanh(z)\n",
    "def tanh_derivative(z): return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def softmax(z):  # Stable softmax\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Loss Functions\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def cross_entropy(y_true, y_pred):  # y_true is one-hot\n",
    "    y_pred = np.clip(y_pred, 1e-9, 1)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Activation registry\n",
    "activation_functions = {\n",
    "    \"relu\": (relu, relu_derivative),\n",
    "    \"sigmoid\": (sigmoid, sigmoid_derivative),\n",
    "    \"tanh\": (tanh, tanh_derivative),\n",
    "    \"softmax\": (softmax, None)  # derivative handled separately\n",
    "}\n",
    "\n",
    "# Neural Network Class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, output_activation=\"softmax\", loss=\"cross_entropy\", lr=0.01):\n",
    "        self.layers = layers  # list of sizes [input, hidden1, ..., output]\n",
    "        self.activations = activations\n",
    "        self.output_activation = output_activation\n",
    "        self.loss_function = cross_entropy if loss == \"cross_entropy\" else binary_cross_entropy\n",
    "        self.lr = lr\n",
    "        self.params = {}\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.params[f\"W{i+1}\"] = np.random.randn(self.layers[i], self.layers[i+1]) * 0.01\n",
    "            self.params[f\"b{i+1}\"] = np.zeros((1, self.layers[i+1]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = {\"A0\": X}\n",
    "        for i in range(1, len(self.layers)):\n",
    "            W = self.params[f\"W{i}\"]\n",
    "            b = self.params[f\"b{i}\"]\n",
    "            Z = np.dot(self.cache[f\"A{i-1}\"], W) + b\n",
    "            act_name = self.activations[i-1] if i != len(self.layers) - 1 else self.output_activation\n",
    "            act_func = activation_functions[act_name][0]\n",
    "            self.cache[f\"Z{i}\"] = Z\n",
    "            self.cache[f\"A{i}\"] = act_func(Z)\n",
    "        return self.cache[f\"A{len(self.layers) - 1}\"]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        grads = {}\n",
    "        L = len(self.layers) - 1\n",
    "        m = Y.shape[0]\n",
    "        A_final = self.cache[f\"A{L}\"]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        if self.output_activation == \"softmax\":\n",
    "            dZ = A_final - Y\n",
    "        elif self.output_activation == \"sigmoid\":\n",
    "            dZ = A_final - Y\n",
    "\n",
    "        for i in reversed(range(1, L + 1)):\n",
    "            A_prev = self.cache[f\"A{i-1}\"]\n",
    "            W = self.params[f\"W{i}\"]\n",
    "            grads[f\"dW{i}\"] = np.dot(A_prev.T, dZ) / m\n",
    "            grads[f\"db{i}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            if i > 1:\n",
    "                Z_prev = self.cache[f\"Z{i-1}\"]\n",
    "                act_deriv = activation_functions[self.activations[i-2]][1]\n",
    "                dA_prev = np.dot(dZ, W.T)\n",
    "                dZ = dA_prev * act_deriv(Z_prev)\n",
    "\n",
    "        # Gradient descent update\n",
    "        for i in range(1, L + 1):\n",
    "            self.params[f\"W{i}\"] -= self.lr * grads[f\"dW{i}\"]\n",
    "            self.params[f\"b{i}\"] -= self.lr * grads[f\"db{i}\"]\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        return self.loss_function(Y, Y_hat)\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            Y_hat = self.forward(X)\n",
    "            loss = self.compute_loss(Y, Y_hat)\n",
    "            self.backward(Y)\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        if self.output_activation == \"softmax\":\n",
    "            return np.argmax(probs, axis=1)\n",
    "        else:\n",
    "            return (probs > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba30d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.1075\n",
      "Epoch 200, Loss: 0.0536\n",
      "Epoch 300, Loss: 0.0352\n",
      "Epoch 400, Loss: 0.0260\n",
      "Epoch 500, Loss: 0.0205\n",
      "Prediction: [[1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 0.2, 0.1]])\n",
    "Y = np.array([[1]])\n",
    "\n",
    "nn = NeuralNetwork(layers=[3, 4, 1], activations=[\"relu\"], output_activation=\"sigmoid\", loss=\"binary_cross_entropy\", lr=0.1)\n",
    "nn.train(X, Y, epochs=500)\n",
    "print(\"Prediction:\", nn.predict(X))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
